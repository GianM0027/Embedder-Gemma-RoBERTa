{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma + RoBERTa Embedder\nThis notebook aims to evaluate the effectiveness of a pipeline that generates embeddings using a large language model (LLM) as a decoder (specifically, gemma_2b_en) and a feed-forward layer to ensure compatibility with the input dimensions of a sentence-level model (SLM) used as an encoder (such as RoBERTa base). We named the full embedding pipeline \"Frankenstein Model\".\nAn extensive set of experiments has been conducted on two datasets: \n1. Amazon Counterfactual Classification: binary classification task.\n2. Emotion Classification: multilabel (6) classification task. \n\nThe experiments gave us the following results:\n\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\textbf{Model} & \\textbf{Amazon Counterfactual Classification (Accuracy, F1)} & \\textbf{Emotion Classification (Accuracy, F1)} & \\textbf{Layer Number} \\\\\n\\hline\n\\text{Roberta-Only} & (0.92358, 0.88786) & (0.84130, 0.79769) & - \\\\\n\\text{Gemma-Only} & (0.65224, 0.59531) & (0.28535, 0.24303) & - \\\\\n\\text{Frankenstein} & (0.59567, 0.53944) & (0.32005, 0.20509) & -1 \\\\\n\\text{Frankenstein} & (0.73567, 0.67447) & (0.29070, 0.23099) & -2 \\\\\n\\text{Frankenstein} & (0.78910, 0.73468) & (0.33010, 0.25578) & -3 \\\\\n\\text{Frankenstein} & (0.81881, 0.75857) & (0.33950, 0.27807) & -4 \\\\\n\\text{Frankenstein} & (0.83522, 0.77633) & (0.17060, 0.11551) & -5 \\\\\n\\text{Frankenstein} & (0.82284, 0.76580) & (0.35050, 0.28033) & -6 \\\\\n\\text{Frankenstein} & (0.84746, 0.79004) & (0.21150, 0.12383) & -7 \\\\\n\\text{Frankenstein} & (0.73687, 0.66874) & (0.21770, 0.17535) & -8 \\\\\n\\text{Frankenstein} & (0.85896, 0.80996) & (0.35980, 0.28138) & -9 \\\\\n\\text{Frankenstein} & (0.82060, 0.76040) & (0.33605, 0.26878) & -10 \\\\\n\\text{Frankenstein} & (0.77179, 0.72740) & (0.29690, 0.24354) & -11 \\\\\n\\text{Frankenstein} & (0.78119, 0.71965) & (0.39920, 0.32163) & -12 \\\\\n\\text{Frankenstein} & (0.80448, 0.75166) & (0.24650, 0.19516) & -13 \\\\\n\\text{Frankenstein} & (0.85119, 0.79530) & (0.39890, 0.32221) & -14 \\\\\n\\text{Frankenstein} & (0.87493, 0.82954) & (0.18935, 0.14127) & -15 \\\\\n\\text{Frankenstein} & (0.82194, 0.76370) & (0.19300, 0.13079) & -16 \\\\\n\\text{Frankenstein} & (0.77567, 0.71621) & (0.18550, 0.14802) & -17 \\\\\n\\text{Frankenstein} & (0.65090, 0.58423) & (0.32005, 0.20508) & -18 \\\\\n\\hline\n\\end{array}\n\nIn this table, the 'Layer Number' column specifies from which layer the output of Gemma is being extracted. Negative numbers indicate the layer count starting from the topmost layer downward. The rows labeled 'Roberta-Only' and 'Gemma-Only' serve as baseline comparisons, demonstrating the performance of the standalone models without integration.\n\nIn order to reproduce these results (or even test the embedders on new datasets) it is sufficient to modify the constants in the following cell:","metadata":{}},{"cell_type":"code","source":"\"\"\"\nPossible tasks to test:\n    \"AmazonCounterfactualClassification\",\n    \"AmazonPolarityClassification\",\n    \"AmazonReviewsClassification\",\n    \"Banking77Classification\",\n    \"EmotionClassification\",\n    \"ImdbClassification\",\n    \"MassiveIntentClassification\",\n    \"MassiveScenarioClassification\",\n    \"MTOPDomainClassification\",\n    \"MTOPIntentClassification\",\n    \"ToxicConversationsClassification\",\n    \"TweetSentimentExtractionClassification\"\n\"\"\"\n\n# default value = 33\nSEED = 33 \n\n# training hyperparameters\nEPOCHS = 5\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-5\nLLM_layers = [-17]\n\nONLY_LLM = False # True if you want to use Gemma-only model\nONLY_SLM = True # True if you want to use Roberta-only model\n \n# Choose only one task from the list above\nTASK = \"EmotionClassification\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ensure compatibility with accelerate and bitsandbytes\n!pip install transformers==4.30\n\n# default required installations\n!pip install mteb\n!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n!pip install torchinfo\n!pip install bitsandbytes\n!pip install accelerate\n!pip install gputil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom mteb import MTEB\nimport warnings\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport random\n\nimport shutil\nfrom IPython.display import FileLink\nimport zipfile\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torchinfo import summary\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.optim as optim\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\nfrom transformers import PreTrainedTokenizer\n\nfrom kaggle_secrets import UserSecretsClient\n\n# import valid hugging_face token (update secret on Kaggle with your token)\nuser_secrets = UserSecretsClient()\nHUGGING_FACE_TOKEN = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\n\n# Suppress Warning that asks if the datasets are reliable\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Suppress Warning that claims slow training and inference during the fitting\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npooling = 'mean'\nVAL_SPLIT = 0.2\n\n# SEED settings to ensure reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Import\nLoading the tasks and storing the corresponding dataset in a vocabulary (as datasets.dataset_dict.DatasetDict objects)","metadata":{}},{"cell_type":"code","source":"datasets_vocabulary = {}\nevaluation_pipeline = MTEB(tasks=[TASK], task_langs=[\"en\"])\n\ndef extract_dataset(datasets_vocabulary, dataset_name, val_split):\n    dataset_dict = datasets_vocabulary[dataset_name].get(\"en\", datasets_vocabulary[dataset_name])\n    \n    test_df = pd.DataFrame(dataset_dict[\"test\"])\n    train_data = pd.DataFrame(dataset_dict[\"train\"])\n\n    # Check if there's a predefined validation set\n    if \"validation\" in dataset_dict:\n        val_df = pd.DataFrame(dataset_dict[\"validation\"])\n    else:\n        # Create a validation set if it doesn't exist\n        train_df, val_df = train_test_split(train_data, test_size=val_split, random_state=SEED)\n        return train_df, val_df, test_df\n    \n    # Use the entire train_data as train_df if validation set exists\n    train_df = train_data\n    return train_df, val_df, test_df\n\nfor task, task_name in zip(evaluation_pipeline.tasks, [TASK]):\n  task.load_data(trust_remote_code=True)\n  datasets_vocabulary[task_name] = task.dataset\n\ntrain_df, val_df, test_df = extract_dataset(datasets_vocabulary, TASK, VAL_SPLIT)\n\nnum_of_labels = len(np.unique(pd.concat([train_df[\"label\"], val_df[\"label\"], test_df[\"label\"]])))\nN_CLASSES = 1 if num_of_labels == 2 else num_of_labels\n\nprint(f\"\\nDataset - {TASK}\")\nprint(f\"Size of dataframes:\\t train - {len(train_df)}\\t validation - {len(val_df)}\\t test - {len(test_df)}\")\nprint(f\"This dataset has {N_CLASSES+1 if num_of_labels == 2 else N_CLASSES} different classes \\n\")\ndisplay(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download of Gemma and RoBERTa","metadata":{}},{"cell_type":"code","source":"gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_auth_token=HUGGING_FACE_TOKEN)\nroberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\nTOKENIZER = roberta_tokenizer if ONLY_SLM else gemma_tokenizer\n\n# Define the quantization configuration for 4-bit loading\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    quantization_dtype=torch.float32,\n    compute_dtype=torch.float32\n)\n\n# Load gemma_model with 4-bit precision\ngemma_model = AutoModelForCausalLM.from_pretrained(\n    \"google/gemma-2b\",\n    quantization_config=quantization_config,\n    use_auth_token=HUGGING_FACE_TOKEN,\n    device_map=\"auto\"\n)\n\nroberta_model = AutoModel.from_pretrained(\"roberta-base\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models Definition","metadata":{}},{"cell_type":"code","source":"# superclass of Frankeinstein Model, Gemma_only and RoBERTa_only\nclass BaseModel(nn.Module):\n    def __init__(self):\n        super(BaseModel, self).__init__()\n    \n    def forward(self, *input, **kwargs):\n        raise NotImplementedError(\"This method should be implemented by subclasses.\")\n\n    def set_encoder_parameters(self, batch_size, max_length, tokenizer, device):\n        self.batch_size = batch_size\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        self.device = device\n        \n    def encode(self, sentences: list[str], **kwargs):\n        try:\n            self.batch_size\n        except AttributeError as e:\n            print(f\"ERROR: {e}\")\n            print(\"Before running the evaluation set its parameters with the function set_encoder_parameters(batch_size, max_length, tokenizer, device)\")\n        self.to(device)\n        max_length = min(self.max_length, self.tokenizer.model_max_length)\n        all_embeddings = []\n        for i in range(0, len(sentences), self.batch_size):\n            batch_sentences = sentences[i:i + self.batch_size]\n            inputs = self.tokenizer(batch_sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n            input_ids = inputs['input_ids'].to(self.device)\n            attention_mask = inputs['attention_mask'].to(self.device)\n \n            with torch.no_grad():\n                embeddings = self(input_ids=input_ids, attention_mask=attention_mask, clf_head=False)\n \n            all_embeddings.append(embeddings.cpu())\n        all_embeddings = torch.cat(all_embeddings, dim=0)\n        return all_embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FrankensteinModel(BaseModel):\n    def __init__(self, large_language_model, small_language_model, n_classes, LLM_layers = -1, pooling = 'max'):\n        super(FrankensteinModel, self).__init__()\n        self.LLM_layers = LLM_layers\n        self.pooling = pooling\n        self.llm = large_language_model\n        self.slm = small_language_model\n        self.projection = nn.Linear(self.llm.config.hidden_size, self.slm.config.hidden_size)\n        self.clf_head = nn.Linear(self.slm.config.hidden_size, n_classes)\n        \n        for param in self.llm.parameters():\n            param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask=None, clf_head = True):\n        llm_outputs = self.llm(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n        if isinstance(self.LLM_layers, int):\n            llm_features = llm_outputs.hidden_states[self.LLM_layers]\n        elif isinstance(self.LLM_layers, list):\n            # Combine multiple layers (mean or max pooling)\n            layers = [llm_outputs.hidden_states[layer] for layer in self.LLM_layers]\n            if self.pooling == 'mean':\n                llm_features = torch.mean(torch.stack(layers), dim=0)\n            elif self.pooling == 'max':\n                llm_features, _ = torch.max(torch.stack(layers), dim=0)\n            else:\n                raise ValueError(f\"Invalid pooling method: {pooling}\")\n        else:\n            raise ValueError(f\"Invalid type for use_layer: {type(use_layer)}. Must be int or list of int.\")\n        llm_features = llm_features.to(torch.float32)\n            \n        projected_features = self.projection(llm_features)\n        slm_outputs = self.slm(inputs_embeds=projected_features, attention_mask=attention_mask)\n        pooled_output = torch.mean(slm_outputs.last_hidden_state, dim=1)\n        \n        if clf_head:\n            output = self.clf_head(pooled_output)\n        else:\n            output = pooled_output\n        return output\n\n    \nclass RoBERTa_only(BaseModel):\n    def __init__(self, small_language_model, n_classes):\n        super(RoBERTa_only, self).__init__()\n        \n        self.slm = small_language_model\n        self.clf_head = nn.Linear(self.slm.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask=None, clf_head=True):\n        slm_outputs = self.slm(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = torch.mean(slm_outputs.last_hidden_state, dim=1)\n        \n        if clf_head:\n            output = self.clf_head(pooled_output)\n        else:\n            output = pooled_output\n        return output\n    \n    \nclass Gemma_only(BaseModel):\n    def __init__(self, large_language_model, n_classes, LLM_layers = -1, pooling = 'max'):\n        super(Gemma_only, self).__init__()\n        self.LLM_layers = LLM_layers\n        self.pooling = pooling\n        self.llm = large_language_model\n        self.clf_head = nn.Linear(self.llm.config.hidden_size, n_classes)\n        \n        for param in self.llm.parameters():\n            param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask=None, clf_head = True):\n        llm_outputs = self.llm(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n        if isinstance(self.LLM_layers, int):\n            llm_features = llm_outputs.hidden_states[self.LLM_layers]\n        elif isinstance(self.LLM_layers, list):\n            # Combine multiple layers (mean or max pooling)\n            layers = [llm_outputs.hidden_states[layer] for layer in self.LLM_layers]\n            if self.pooling == 'mean':\n                llm_features = torch.mean(torch.stack(layers), dim=0)\n            elif self.pooling == 'max':\n                llm_features, _ = torch.max(torch.stack(layers), dim=0)\n            else:\n                raise ValueError(f\"Invalid pooling method: {pooling}\")\n        else:\n            raise ValueError(f\"Invalid type for use_layer: {type(use_layer)}. Must be int or list of int.\")\n        llm_features = llm_features.to(torch.float32)\n            \n        pooled_output = torch.mean(llm_features, dim=1)\n        \n        if clf_head:\n            output = self.clf_head(pooled_output)\n        else:\n            output = pooled_output\n        return output\n    \n    \nif ONLY_LLM:\n    model = Gemma_only(large_language_model=gemma_model, n_classes=N_CLASSES)\nelif ONLY_SLM:\n    model = RoBERTa_only(small_language_model=roberta_model, n_classes=N_CLASSES)\nelse: \n    model = FrankensteinModel(large_language_model=gemma_model, small_language_model=roberta_model, n_classes=N_CLASSES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataloaders creation ","metadata":{}},{"cell_type":"code","source":"def find_max_encoded_utterance_len(data, tokenizer=TOKENIZER):    \n    max_length = max([len(tokenizer.encode_plus(sentence)[\"input_ids\"]) for sentence in data])\n    return max_length\n\n\ndef create_dataloader(df, tokenizer=TOKENIZER, n_classes = N_CLASSES, batch_size = BATCH_SIZE, x=\"text\", y=\"label\", shuffle = True, max_length=512):\n    texts = df[x].tolist()\n    labels = df[y].tolist()\n    max_length = min(max_length, tokenizer.model_max_length)\n    \n    tokens = tokenizer.batch_encode_plus(\n        texts,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length,\n        return_tensors='pt'\n    )\n    \n    # Create a TensorDataset\n    dataset = TensorDataset(tokens['input_ids'], tokens['attention_mask'], torch.tensor(labels))\n    \n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n\ntrain_max_tokenized_length = find_max_encoded_utterance_len(train_df[\"text\"])\nval_max_tokenized_length = find_max_encoded_utterance_len(val_df[\"text\"])\ntest_max_tokenized_length = find_max_encoded_utterance_len(test_df[\"text\"])\n\n# creating dataloader for ToxicConversationsClassification binary task\ntrain_dataloader = create_dataloader(train_df, shuffle=True, max_length=train_max_tokenized_length)\nval_dataloader = create_dataloader(val_df, shuffle=False, max_length=val_max_tokenized_length)\ntest_dataloader = create_dataloader(test_df, shuffle=False, max_length=test_max_tokenized_length)\n\n\nfor dataloader, name in zip([train_dataloader, val_dataloader, test_dataloader], [\"train\", \"val\", \"test\"]):\n    for input_ids, attention_mask, labels in dataloader:\n        print(f\"Shape of {name} batch:\", input_ids.shape)\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and Evaluate functions ","metadata":{}},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss() if N_CLASSES == 1 else nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=EPOCHS, device=device, n_classes=N_CLASSES):\n    model.to(device)\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct_predictions = 0\n        total_predictions = 0\n\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n\n            optimizer.zero_grad()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n            if n_classes == 1:\n                loss = criterion(outputs.squeeze(), labels.float())\n                predictions = (outputs.squeeze() > 0.5).int()\n            else:\n                loss = criterion(outputs, labels)\n                predictions = torch.argmax(outputs, dim=1)\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * input_ids.size(0)\n            correct_predictions += (predictions == labels).sum().item()\n            total_predictions += labels.size(0)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = correct_predictions / total_predictions\n\n        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device, n_classes)\n\n        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.7f} - Val Loss: {val_loss:.7f} - Val Accuracy: {val_acc:.7f}\")\n\ndef evaluate_model(model, val_loader, criterion, device = device, n_classes = N_CLASSES):\n    model.to(device)\n    \n    model.eval()\n    running_loss = 0.0\n    correct_predictions = 0\n    total_predictions = 0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n            if n_classes == 1:\n                loss = criterion(outputs.squeeze(), labels.float())\n                predictions = (outputs.squeeze() > 0.5).int()\n            else:\n                loss = criterion(outputs, labels)\n                predictions = torch.argmax(outputs, dim=1)\n\n            running_loss += loss.item() * input_ids.size(0)\n            correct_predictions += (predictions == labels).sum().item()\n            total_predictions += labels.size(0)\n\n    epoch_loss = running_loss / len(val_loader.dataset)\n    epoch_acc = correct_predictions / total_predictions\n\n    return epoch_loss, epoch_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training and Testing on MTEB Dataset","metadata":{}},{"cell_type":"code","source":"# Do not train at all if ONLY_LLM\nif not ONLY_LLM:\n    train_model(model, val_dataloader, test_dataloader, criterion, optimizer, num_epochs = EPOCHS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc = evaluate_model(model, test_dataloader, criterion, device, N_CLASSES)\nprint(\" --- Evaluation by using a classification head --- \")\nprint(f\"Loss on the test set: {test_loss}\")\nprint(f\"Accuracy on the test set: {test_acc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.set_encoder_parameters(BATCH_SIZE, test_max_tokenized_length, TOKENIZER, device)\nevaluation = evaluation_pipeline.run(model,\n                                     eval_splits=[\"test\"],\n                                     output_folder=\"results\",\n                                     overwrite_results=True)\n\nprint(\" --- MTEB Evaluation --- \")\nprint(\"Average accuracy\", evaluation[0].scores[\"test\"][0][\"accuracy\"])\nprint(\"Average f1\", evaluation[0].scores[\"test\"][0][\"f1\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}