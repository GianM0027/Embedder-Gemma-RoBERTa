{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma + RoBERTa Embedder\n",
        "\n",
        "This notebook aims to evaluate the effectiveness of a pipeline that generates embeddings using a large language model (LLM) as a decoder (specifically, `gemma_2b_en`) and a feed-forward layer to ensure compatibility with the input dimensions of a sentence-level model (SLM) used as an encoder (such as `RoBERTa base`). We named the full embedding pipeline the \"Frankenstein Model\".\n",
        "\n",
        "Each execution of this notebook computes the results for a single configuration. A configuration is characterized by:\n",
        "*   Dataset under analysis (listed in the cell below).\n",
        "*   Model being used (Frankenstein, Gemma_only, RoBERTa_only).\n",
        "*   Whether the model under analysis is trained or not.\n",
        "\n",
        "At the end of the notebook, after execution, the accuracy and F1 score on the test set are printed, highlighting the effectiveness of the embeddings produced for the given configuration. To reproduce the results described in the attached report (or to test the embedders on new datasets), simply modify the constants in the following cell:"
      ],
      "metadata": {
        "id": "gmyfLqoyWL30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Possible tasks to test:\n",
        "    \"AmazonCounterfactualClassification\",\n",
        "    \"AmazonPolarityClassification\",\n",
        "    \"AmazonReviewsClassification\",\n",
        "    \"Banking77Classification\",\n",
        "    \"EmotionClassification\",\n",
        "    \"ImdbClassification\",\n",
        "    \"MassiveIntentClassification\",\n",
        "    \"MassiveScenarioClassification\",\n",
        "    \"MTOPDomainClassification\",\n",
        "    \"MTOPIntentClassification\",\n",
        "    \"ToxicConversationsClassification\",\n",
        "    \"TweetSentimentExtractionClassification\"\n",
        "\"\"\"\n",
        "\n",
        "# default value = 33\n",
        "SEED = 33\n",
        "\n",
        "# training hyperparameters\n",
        "TRAIN_MODEL = True\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-5\n",
        "LLM_layers = -1\n",
        "\n",
        "ONLY_LLM = False # True if you want to use Gemma-only model\n",
        "ONLY_SLM = False # True if you want to use Roberta-only model\n",
        "\n",
        "# Choose only one task from the list above\n",
        "TASK = \"AmazonCounterfactualClassification\""
      ],
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "ZbsQT0D3WL32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure compatibility with accelerate and bitsandbytes\n",
        "!pip install transformers==4.30\n",
        "\n",
        "# default required installations\n",
        "!pip install mteb\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torchinfo\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install gputil"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "sMJ1U1iHWL32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from mteb import MTEB\n",
        "import warnings\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import bitsandbytes as bnb\n",
        "import random\n",
        "\n",
        "import shutil\n",
        "from IPython.display import FileLink\n",
        "import zipfile\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torchinfo import summary\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# import valid hugging_face token (update secret on Kaggle with your token)\n",
        "user_secrets = UserSecretsClient()\n",
        "HUGGING_FACE_TOKEN = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\n",
        "\n",
        "# Suppress Warning that asks if the datasets are reliable\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Suppress Warning that claims slow training and inference during the fitting\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pooling = 'mean'\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "# SEED settings to ensure reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED);"
      ],
      "metadata": {
        "trusted": true,
        "id": "B4dQ6OReWL33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Import\n",
        "Loading the tasks and storing the corresponding dataset in a vocabulary (as datasets.dataset_dict.DatasetDict objects)"
      ],
      "metadata": {
        "id": "8h8UXl5bWL33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_vocabulary = {}\n",
        "evaluation_pipeline = MTEB(tasks=[TASK], task_langs=[\"en\"])\n",
        "\n",
        "def extract_dataset(datasets_vocabulary, dataset_name, val_split):\n",
        "    dataset_dict = datasets_vocabulary[dataset_name].get(\"en\", datasets_vocabulary[dataset_name])\n",
        "\n",
        "    test_df = pd.DataFrame(dataset_dict[\"test\"])\n",
        "    train_data = pd.DataFrame(dataset_dict[\"train\"])\n",
        "\n",
        "    # Check if there's a predefined validation set\n",
        "    if \"validation\" in dataset_dict:\n",
        "        val_df = pd.DataFrame(dataset_dict[\"validation\"])\n",
        "    else:\n",
        "        # Create a validation set if it doesn't exist\n",
        "        train_df, val_df = train_test_split(train_data, test_size=val_split, random_state=SEED)\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "    # Use the entire train_data as train_df if validation set exists\n",
        "    train_df = train_data\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "for task, task_name in zip(evaluation_pipeline.tasks, [TASK]):\n",
        "  task.load_data(trust_remote_code=True)\n",
        "  datasets_vocabulary[task_name] = task.dataset\n",
        "\n",
        "train_df, val_df, test_df = extract_dataset(datasets_vocabulary, TASK, VAL_SPLIT)\n",
        "\n",
        "num_of_labels = len(np.unique(pd.concat([train_df[\"label\"], val_df[\"label\"], test_df[\"label\"]])))\n",
        "N_CLASSES = 1 if num_of_labels == 2 else num_of_labels\n",
        "\n",
        "print(f\"\\nDataset - {TASK}\")\n",
        "print(f\"Size of dataframes:\\t train - {len(train_df)}\\t validation - {len(val_df)}\\t test - {len(test_df)}\")\n",
        "print(f\"This dataset has {N_CLASSES+1 if num_of_labels == 2 else N_CLASSES} different classes \\n\")\n",
        "display(train_df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bNWPUwIkWL33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download of Gemma and RoBERTa"
      ],
      "metadata": {
        "id": "_eV8UninWL34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_auth_token=HUGGING_FACE_TOKEN)\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "TOKENIZER = roberta_tokenizer if ONLY_SLM else gemma_tokenizer\n",
        "\n",
        "# Define the quantization configuration for 4-bit loading\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    quantization_dtype=torch.float32,\n",
        "    compute_dtype=torch.float32\n",
        ")\n",
        "\n",
        "# Load gemma_model with 4-bit precision\n",
        "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-2b\",\n",
        "    quantization_config=quantization_config,\n",
        "    use_auth_token=HUGGING_FACE_TOKEN,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "roberta_model = AutoModel.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "BKdK4Gs5WL34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Definition"
      ],
      "metadata": {
        "id": "PRysrizPWL34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# superclass of Frankeinstein Model, Gemma_only and RoBERTa_only\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseModel, self).__init__()\n",
        "\n",
        "    def forward(self, *input, **kwargs):\n",
        "        raise NotImplementedError(\"This method should be implemented by subclasses.\")\n",
        "\n",
        "    def set_encoder_parameters(self, batch_size, max_length, tokenizer, device):\n",
        "        self.batch_size = batch_size\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def encode(self, sentences: list[str], **kwargs):\n",
        "        try:\n",
        "            self.batch_size\n",
        "        except AttributeError as e:\n",
        "            print(f\"ERROR: {e}\")\n",
        "            print(\"Before running the evaluation set its parameters with the function set_encoder_parameters(batch_size, max_length, tokenizer, device)\")\n",
        "        self.to(device)\n",
        "        max_length = min(self.max_length, self.tokenizer.model_max_length)\n",
        "        all_embeddings = []\n",
        "        for i in range(0, len(sentences), self.batch_size):\n",
        "            batch_sentences = sentences[i:i + self.batch_size]\n",
        "            inputs = self.tokenizer(batch_sentences, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "            input_ids = inputs['input_ids'].to(self.device)\n",
        "            attention_mask = inputs['attention_mask'].to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embeddings = self(input_ids=input_ids, attention_mask=attention_mask, clf_head=False)\n",
        "\n",
        "            all_embeddings.append(embeddings.cpu())\n",
        "        all_embeddings = torch.cat(all_embeddings, dim=0)\n",
        "        return all_embeddings"
      ],
      "metadata": {
        "trusted": true,
        "id": "7kApTZ83WL34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FrankensteinModel(BaseModel):\n",
        "    def __init__(self, large_language_model, small_language_model, n_classes, LLM_layers = -1, pooling = 'max'):\n",
        "        super(FrankensteinModel, self).__init__()\n",
        "        self.LLM_layers = LLM_layers\n",
        "        self.pooling = pooling\n",
        "        self.llm = large_language_model\n",
        "        self.slm = small_language_model\n",
        "        self.projection = nn.Linear(self.llm.config.hidden_size, self.slm.config.hidden_size)\n",
        "        self.clf_head = nn.Linear(self.slm.config.hidden_size, n_classes)\n",
        "\n",
        "        for param in self.llm.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, clf_head = True):\n",
        "        llm_outputs = self.llm(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        if isinstance(self.LLM_layers, int):\n",
        "            llm_features = llm_outputs.hidden_states[self.LLM_layers]\n",
        "        elif isinstance(self.LLM_layers, list):\n",
        "            # Combine multiple layers (mean or max pooling)\n",
        "            layers = [llm_outputs.hidden_states[layer] for layer in self.LLM_layers]\n",
        "            if self.pooling == 'mean':\n",
        "                llm_features = torch.mean(torch.stack(layers), dim=0)\n",
        "            elif self.pooling == 'max':\n",
        "                llm_features, _ = torch.max(torch.stack(layers), dim=0)\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid pooling method: {pooling}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid type for use_layer: {type(use_layer)}. Must be int or list of int.\")\n",
        "        llm_features = llm_features.to(torch.float32)\n",
        "\n",
        "        projected_features = self.projection(llm_features)\n",
        "        slm_outputs = self.slm(inputs_embeds=projected_features, attention_mask=attention_mask)\n",
        "        pooled_output = torch.mean(slm_outputs.last_hidden_state, dim=1)\n",
        "\n",
        "        if clf_head:\n",
        "            output = self.clf_head(pooled_output)\n",
        "        else:\n",
        "            output = pooled_output\n",
        "        return output\n",
        "\n",
        "\n",
        "class RoBERTa_only(BaseModel):\n",
        "    def __init__(self, small_language_model, n_classes):\n",
        "        super(RoBERTa_only, self).__init__()\n",
        "\n",
        "        self.slm = small_language_model\n",
        "        self.clf_head = nn.Linear(self.slm.config.hidden_size, n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, clf_head=True):\n",
        "        slm_outputs = self.slm(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = torch.mean(slm_outputs.last_hidden_state, dim=1)\n",
        "\n",
        "        if clf_head:\n",
        "            output = self.clf_head(pooled_output)\n",
        "        else:\n",
        "            output = pooled_output\n",
        "        return output\n",
        "\n",
        "\n",
        "class Gemma_only(BaseModel):\n",
        "    def __init__(self, large_language_model, n_classes, LLM_layers = -1, pooling = 'max'):\n",
        "        super(Gemma_only, self).__init__()\n",
        "        self.LLM_layers = LLM_layers\n",
        "        self.pooling = pooling\n",
        "        self.llm = large_language_model\n",
        "        self.clf_head = nn.Linear(self.llm.config.hidden_size, n_classes)\n",
        "\n",
        "        for param in self.llm.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, clf_head = True):\n",
        "        llm_outputs = self.llm(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        if isinstance(self.LLM_layers, int):\n",
        "            llm_features = llm_outputs.hidden_states[self.LLM_layers]\n",
        "        elif isinstance(self.LLM_layers, list):\n",
        "            # Combine multiple layers (mean or max pooling)\n",
        "            layers = [llm_outputs.hidden_states[layer] for layer in self.LLM_layers]\n",
        "            if self.pooling == 'mean':\n",
        "                llm_features = torch.mean(torch.stack(layers), dim=0)\n",
        "            elif self.pooling == 'max':\n",
        "                llm_features, _ = torch.max(torch.stack(layers), dim=0)\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid pooling method: {pooling}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid type for use_layer: {type(use_layer)}. Must be int or list of int.\")\n",
        "\n",
        "        llm_features = llm_features.to(torch.float32)\n",
        "        pooled_output = torch.mean(llm_features, dim=1)\n",
        "\n",
        "        if clf_head:\n",
        "            output = self.clf_head(pooled_output)\n",
        "        else:\n",
        "            output = pooled_output\n",
        "        return output\n",
        "\n",
        "\n",
        "if ONLY_LLM:\n",
        "    model = Gemma_only(large_language_model=gemma_model, n_classes=N_CLASSES, LLM_layers = LLM_layers, pooling = pooling)\n",
        "elif ONLY_SLM:\n",
        "    model = RoBERTa_only(small_language_model=roberta_model, n_classes=N_CLASSES)\n",
        "else:\n",
        "    model = FrankensteinModel(large_language_model=gemma_model, small_language_model=roberta_model, n_classes=N_CLASSES, LLM_layers = LLM_layers, pooling = pooling)"
      ],
      "metadata": {
        "trusted": true,
        "id": "y1tolf5FWL35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloaders creation"
      ],
      "metadata": {
        "id": "ArjvJCtxWL35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_max_encoded_utterance_len(data, tokenizer=TOKENIZER):\n",
        "    max_length = max([len(tokenizer.encode_plus(sentence)[\"input_ids\"]) for sentence in data])\n",
        "    return max_length\n",
        "\n",
        "def create_dataloader(df, tokenizer=TOKENIZER, n_classes=N_CLASSES, batch_size=BATCH_SIZE, x=\"text\", y=\"label\", shuffle=True, max_length=512):\n",
        "    texts = df[x].tolist()\n",
        "\n",
        "    # Convert labels to integers if they are not already\n",
        "    if df[y].dtype == 'object':\n",
        "        labels, _ = pd.factorize(df[y])\n",
        "    else:\n",
        "        labels = df[y].tolist()\n",
        "\n",
        "    max_length = min(max_length, tokenizer.model_max_length)\n",
        "\n",
        "    tokens = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    dataset = TensorDataset(tokens['input_ids'], tokens['attention_mask'], torch.tensor(labels, dtype=torch.long))\n",
        "\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "train_max_tokenized_length = find_max_encoded_utterance_len(train_df[\"text\"])\n",
        "val_max_tokenized_length = find_max_encoded_utterance_len(val_df[\"text\"])\n",
        "test_max_tokenized_length = find_max_encoded_utterance_len(test_df[\"text\"])\n",
        "\n",
        "train_dataloader = create_dataloader(train_df, shuffle=True, max_length=train_max_tokenized_length)\n",
        "val_dataloader = create_dataloader(val_df, shuffle=False, max_length=val_max_tokenized_length)\n",
        "test_dataloader = create_dataloader(test_df, shuffle=False, max_length=test_max_tokenized_length)\n",
        "\n",
        "\n",
        "for dataloader, name in zip([train_dataloader, val_dataloader, test_dataloader], [\"train\", \"val\", \"test\"]):\n",
        "    for input_ids, attention_mask, labels in dataloader:\n",
        "        print(f\"Shape of {name} batch:\", input_ids.shape)\n",
        "        break"
      ],
      "metadata": {
        "trusted": true,
        "id": "kUJdIul3WL35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate functions"
      ],
      "metadata": {
        "id": "jSecnfEtWL35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss() if N_CLASSES == 1 else nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=EPOCHS, device=device, n_classes=N_CLASSES):\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
        "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            if n_classes == 1:\n",
        "                loss = criterion(outputs.squeeze(), labels.float())\n",
        "                predictions = (outputs.squeeze() > 0.5).int()\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "                predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * input_ids.size(0)\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct_predictions / total_predictions\n",
        "\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device, n_classes)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.7f} - Val Loss: {val_loss:.7f} - Val Accuracy: {val_acc:.7f}\")\n",
        "\n",
        "def evaluate_model(model, val_loader, criterion, device = device, n_classes = N_CLASSES):\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            if n_classes == 1:\n",
        "                loss = criterion(outputs.squeeze(), labels.float())\n",
        "                predictions = (outputs.squeeze() > 0.5).int()\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "                predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            running_loss += loss.item() * input_ids.size(0)\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    epoch_acc = correct_predictions / total_predictions\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "trusted": true,
        "id": "PghqPGUIWL35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testing on MTEB Task"
      ],
      "metadata": {
        "id": "kWSMMaEtWL36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train only if TRAIN_MODEL is true and this is not the Gemma_only configuration\n",
        "if TRAIN_MODEL and (not ONLY_LLM):\n",
        "    train_model(model, val_dataloader, test_dataloader, criterion, optimizer, num_epochs = EPOCHS)"
      ],
      "metadata": {
        "trusted": true,
        "id": "RRFcIH1GWL36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.set_encoder_parameters(BATCH_SIZE, test_max_tokenized_length, TOKENIZER, device)\n",
        "evaluation = evaluation_pipeline.run(model,\n",
        "                                     eval_splits=[\"test\"],\n",
        "                                     output_folder=\"results\",\n",
        "                                     overwrite_results=True)\n",
        "\n",
        "print(\" --- MTEB Evaluation --- \")\n",
        "print(\"Average accuracy\", evaluation[0].scores[\"test\"][0][\"accuracy\"])\n",
        "print(\"Average f1\", evaluation[0].scores[\"test\"][0][\"f1\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "jxxZojPaWL36"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}